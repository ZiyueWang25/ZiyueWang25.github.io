<!DOCTYPE html>
<html lang="en">

  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>My thoughts on AI and personal future plan after learning about AI Safety for 4 months | Ziyue  Wang</title>
    <meta name="author" content="Ziyue  Wang">
    <meta name="description" content="About AI promising and also dangerous future and what I want to do about it.">
    <meta name="keywords" content="portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/monkey.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://ziyuewang25.github.io/blog/2023/AI-Safety/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ziyue </span>Wang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">My thoughts on AI and personal future plan after learning about AI Safety for 4 months</h1>
    <p class="post-meta">August 30, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/project">
          <i class="fas fa-hashtag fa-sm"></i> Project</a>  
          
        ·  
        <a href="/blog/category/ai">
          <i class="fas fa-tag fa-sm"></i> AI</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>Cross-posted on <a href="https://www.lesswrong.com/posts/GreaiJYjFDziXbfCw/my-thoughts-on-ai-and-personal-future-plan-after-learning" rel="external nofollow noopener" target="_blank">LessWrong</a>.</p>

<p>In this post, I want to distill some of my thoughts about AI and my future plan regarding it according to what I have learnt during the past 3~4 months.</p>

<p>Overall, I think the future of AI is promising but in the same dangerous if they cannot align with our intention. It is like what has been discussed in <a href="https://theprecipice.com/" rel="external nofollow noopener" target="_blank">Precipice</a> and I want to take the chance to help it.</p>

<p>I say it is promising because it already demonstrated superior capability, and can be used to improve people’s life quality. The application field can be robotics, education, health system, productivity boost and etc.</p>

<p>But AI can get misaligned if we aren’t paying enough attention to it. Here, according to Paul Christiano, alignment means <a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6" rel="external nofollow noopener" target="_blank">intent alignment</a>:</p>

<blockquote>
  <p>AI A is aligned with an operator H when A is trying to do what H wants it to do.</p>
</blockquote>

<p>The focus here is “trying”, which means it is on intention/goal/motivation level rather than behavior level. A model can behave like aligned but it tries to <a href="https://www.lesswrong.com/posts/YgAKhkBdgeTCn6P53/ai-deception-a-survey-of-examples-risks-and-potential" rel="external nofollow noopener" target="_blank">deceive human</a> or human actually couldn’t see its defect due to the high complexity of future tasks.</p>

<p>In the argument above, we assumed it can have “intention” and we can understand how it comes from by using optimizer. When we train a model, we use an optimizer which usually tries to minimize certain loss function or maximize reward function by adjusting the weight of the model. For example, Generative Pretraining Transformer (GPT) is trained by minimizing the next token prediction loss. So the goal of it is simply trying to make the next word make sense given what it sees in the past. Its goal is not about aligning with human instruction and making human happy or productive. Hence we need to do further finetuning, like Reinforcement From Human Feedback (RLHF), to make it align with human instruction.</p>

<p>But I am not confident they are aligned by doing this. Here are several reasons:</p>
<ol>
  <li>
<strong>Mesa-optimizer</strong>: The optimizer we used in the model training is <strong>not the same</strong> as the optimizer inside (<a href="https://www.youtube.com/watch?v=bJLcIBixGj8&amp;t=147s&amp;ab_channel=RobertMilesAISafety" rel="external nofollow noopener" target="_blank">mesa-optimizer</a>) the model that drives its behavior. We can use a dataset to teach the model to tell the truth but due to labeling mistake, the model can understand it as <strong>telling the result as long as human think it is correct, rather than the truth</strong>. It can also comes from a concept called “<strong><a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view" rel="external nofollow noopener" target="_blank">instrumental convergence</a></strong>”, which means as the model tend to do something, it can also develop some goals to help itself achieve that. Common instrumental goals are self-preservation, power-seeking and etc. For example, it is trained to make the quality of a person better, and it learnt to avoid being shutdown because if it get shut down, it can no longer  make a person life better. So overall, a mesa-optimizer make the intention of the model different from what we want, hence misaligned.</li>
  <li>
<strong>The bulk of capability and hence “intention” are still from pretraining</strong>:  Compared with finetuning, pretraining takes hundreds of more resources to do. During that phase, the model get exposed to a lot of knowledge. It is not told to be polite and helpful during that phase, it is simply told to predict the next word. Putting this mode on human would be like: children are getting “educated” about reasoning, culture, reading comprehension and etc without being told about what is a good intention and how to communicate or help other people. This sounds dangerous because before they are being told about what is a good intention, they can already develop their own view strongly, which may not be in favor of the good intention. This is especially dangerous when the pretraining data, corpus, contains a lot of toxic/biased data. Some arguments against this would be how the model learns is different from human and their intention can be corrected during finetuning phase. But still, there is a chance that they can develop strong intention during pretraining phase, which we has almost no control over, except making the data better.</li>
  <li>
<strong>Many existing jail breaker</strong>: there are many <a href="https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day" rel="external nofollow noopener" target="_blank">examples</a> online showing that model turned into a rude/toxic mode by saying unfavorable things. Those behavior can be elicited by using role-play and some other hacking methods.  They are the sign of bad intention within the model that we currently cannot control with.</li>
</ol>

<p>Even when the model is aligned, it can still be misused. So a model that tries to do what human wants it to do can also be applied to dangerous field, like weapon development, attacking security system and etc. This requires policy maker and corporate to have proper control over how AI gets deployed and used, which leads to a large field of AI safety policy and governance. For more details, please check <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastrophic-ai-risks-summary" rel="external nofollow noopener" target="_blank">An overview of Catastrophic AI Risks</a>.</p>

<p>According to <a href="https://theprecipice.com/" rel="external nofollow noopener" target="_blank">Precipice</a> , misaligned AI can pose existential risk to humanity and the chance about it for this century is around 10%. This a large percentage and may sound wild to you at this moment and you may not be convinced, that’s totally understandable. But even we feel uncertain at this moment, the outcome of this small-chance event is unimaginable. This puts us into urgent state to take action and for me, it is about becoming an <a href="https://80000hours.org/articles/ml-engineering-career-transition-guide/" rel="external nofollow noopener" target="_blank">AI Safety Research Engineer</a> to help model get aligned.</p>

<p>I started learning about different safety topics since May 2023. According to <a href="https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment" rel="external nofollow noopener" target="_blank">the AI landscape</a> from Paul Christiano, there are many topics about AI alignment.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lWrTk7ibP-mUTOL481Gheg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lWrTk7ibP-mUTOL481Gheg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lWrTk7ibP-mUTOL481Gheg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lWrTk7ibP-mUTOL481Gheg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">AI Landscape from Paul Christiano</figcaption>

</figure>

<p>Up to this point, to be honest, I don’t have a strong preference over which direction to go :) But if I had to say one, I think “Inner Alignment” part sounds more important to me because only with inner alignment verification, we can tell whether outer alignment worked or not. The concrete inner alignment examples are like <a href="https://www.anthropic.com/index/core-views-on-ai-safety" rel="external nofollow noopener" target="_blank">Scalable Oversight</a>, <a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html" rel="external nofollow noopener" target="_blank">Mechanistic Interpretability</a>, Automated <a href="https://huggingface.co/blog/red-teaming" rel="external nofollow noopener" target="_blank">Red Teaming</a>, <a href="https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk#:~:text=Eliciting%20Latent%20Knowledge%20is%20an,that%20look%20good%20to%20us." rel="external nofollow noopener" target="_blank">Eliciting Latent Knowledge</a> and etc.</p>

<p>The main point at this point in my life and career is to switch my career towards them and contribute the bulk of my day time to maximize my impacts. I felt motivated given the urgency we have to solve this alignment problem and I look forward to the day of becoming an AI Safety Research Engineer :)</p>

  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/DLC/">Deep Learning Curriculum learning experience</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/hackathon/">Can Large Language Models Solve Security Challenges?</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/DLC-T9-AdversarialTraining/">Red Teaming Language Models with Language Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/DLC-T7-Alignment/">RLHF from Shakespeare</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ARENA/">ARENA learning experience</a>
  </li>

<div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "ziyuewang25/ziyuewang25.github.io",
        "data-repo-id": "R_kgDOJYJ4JQ",
        "data-category": "Announcements",
        "data-category-id": "DIC_kwDOJYJ4Jc4CV2ve",
        "data-mapping": "pathname",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    </div>

    <!-- Footer -->
    <!--    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2023 Ziyue  Wang. 
      </div>
    </footer> -->

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-GF7DRR9GYB"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-GF7DRR9GYB');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
