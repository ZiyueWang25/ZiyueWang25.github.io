---
layout: post
title: ARENA learning experience
date: 2023-08-01
description: I summarized my learning experience about ARENA.
tags: ML
categories: AI
giscus_comments: true

---

I found [ARENA](https://github.com/callummcdougall/ARENA_2.0) quite helpful for self-study AI safety related topics and it can work well in together with [Deep Learning Curriculum](https://github.com/jacobhilton/deep_learning_curriculum). It offers colab choice for the exercise part, which is great since I don't have much GPU support for my own computer.

Here is how I spent my time on various topics in ARENA and hope it can work as a reference for someone also interested in self-studying this material.

Total Hours: 42.7. Split into the following:
- 8.7h [Chapter 0: Fundamentals](https://arena-ch0-fundamentals.streamlit.app/)
	- skipped  exercise 0.1 [Ray Tracing](https://arena-ch0-fundamentals.streamlit.app/[0.1]_Ray_Tracing) since I found it too advanced to be necessary.
	- 2.3h exercise 0.2 [CNN](https://colab.research.google.com/drive/1tmwlA1YQIrgXblzo_9q2mNvEOuAqsV3N?usp=sharing). I learnt about `torch.as_strided` related stuff.
	- 2.4h exercise 0.3 [Resnet](https://colab.research.google.com/drive/1gnUiIAzIvjYvaUXdZP7J8e_n1qeLhHFj?usp=sharing). I learnt a more detailed view about resnet, pytorch_lighting, batchnorm.
	- 2.0h exercise 0.4 [Optimization](https://colab.research.google.com/drive/1HjzmCYqBVz_Q0XVj0mmOZGV26tW1WvHi?usp=sharing). I learnt about details of various optimizer, `Weight & Bias` related usage.
	- 2.0h  exercise 0.5 [Backprop](https://colab.research.google.com/drive/1WjtXIlpr3iC5fPGC4_hGSm6nYMdIptNc?usp=sharing) (skipped part 3 & 4 & 5). I learnt about details of back propagation & Autograd.
- 8.0h [Chapter 1: Transformers & Mech Interp](https://arena-ch1-transformers.streamlit.app/)
	- 3.0h  exercise 1.1 [transformer](https://colab.research.google.com/drive/1Ig779Od-OoO8lHolRqWQTaAvhK98EdJT?usp=sharing) (I skipped part of sampling.). I learnt about details of transformer, sampling, training and inference sampling. 
	- 5.0h  exercise 1.2 [mechanistic interpretability](https://colab.research.google.com/drive/1NfLlt3McxOK9eY4xT_S6Q0ZFaoXtrW2B?usp=sharing). I learnt about induction circuits, transformerLens, induction heads, hooks, reverse-engineering induction circuits. These material opened a new view for me about how to understand LLM. I find some part hard to understand though and skipped some of the exercise as I don't want to spend too much time on this topic for now.
- 16.0h [Chapter 2: Reinforcement Learning](https://arena-ch2-rl.streamlit.app/)
	- 2.5h  exercise 2.1 [Introduction to RL](https://arena-ch2-rl.streamlit.app/). it works like a memory refresher about some RL concepts. It is nice to check the detail of some RL environments.
	- 7.0h  exercise 2.2 [Deep Q Learning](https://arena-ch2-rl.streamlit.app/)
	- 4.0h  exercise 2.3 [PPO](https://colab.research.google.com/drive/1UlhPmIfhQLo_10r5OkDwLxWF--A2iCKc?usp=sharing). The Atari Breakout game result is shown [here](https://wandb.ai//vincentwang25/PPOAtari/reports/videos-23-07-18-13-10-22---Vmlldzo0OTA1MjM0).
	- 2.5h  exercise 2.4 [RLHF](https://arena-ch2-rl.streamlit.app/[2.4]_RLHF).
- 10.0h [Chapter 3 Training at Scale](https://arena-ch3-training-at-scale.streamlit.app/)
	- I spent most of my time on doing the exercise in Data Parallelism part and skipped most of the exercise in other sections.
