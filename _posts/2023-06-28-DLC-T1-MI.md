---
layout: post
title: Find the induction heads in GPT-2
date: 2023-06-28
description: I tried to replicating the Decoder-only transformer by following "Attention is all you need" paper and trained it on William Shakespeare's work.
tags: ML
categories: AI
giscus_comments: true

---

I started following [Deep Learning Curriculum](https://github.com/jacobhilton/deep_learning_curriculum/tree/master) written by [Jacob Hilton](https://www.jacobh.co.uk/) and here is what I learnt from the exercise in [Topic 8 - Interpretability](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/8-Interpretability.md). My solution is written in Colab [T8-Interpretability-solution.ipynb](https://colab.research.google.com/drive/15CSZ09T0LQ4_BAM7_NcGDy5sTVneFJQw?usp=sharing)

It took me around 8 hours to finish the exercise and most of the time is spent on [ARENA](https://github.com/callummcdougall/ARENA_2.0) course material about this topic. It helps me a lot in understanding this topic and it offers many helper functions to do this exercise. Here is what I learnt from this exercise:
1. **Induction head**: A head which implements the induction behavior. They attend to the token immediately after an earlier copy of the current token, and then predicts that the token attended to will come next.
2. How to reverse engineer the model's behavior.
3. How to find induction head by checking the model's attention pattern and doing logit attribution.