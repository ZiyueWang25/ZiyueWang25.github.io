---
layout: post
title: Implementing PPO from scratch
date: 2023-07-23
description: I tried to implementing PPO from scratch and apply it to Procgen environment. Here is what I learnt.
tags: Project
categories: AI
giscus_comments: true

---

I started following [Deep Learning Curriculum](https://github.com/jacobhilton/deep_learning_curriculum/tree/master)(DLC) written by [Jacob Hilton](https://www.jacobh.co.uk/) and here is what I experienced and learnt from the exercise in [Topic 6 - Reinforcement Learning](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/6-Reinforcement-Learning.md). **My solution is written in Colab [T6-RL-solution.ipynb](https://colab.research.google.com/drive/1n8EhT0RHxdS1MIgiPQkvjDX7sD7Mpxoy?usp=sharing)**

It took me around 40 hours to finish the exercise. I started by spending around 15 hours doing the exercise in [ARENA](https://github.com/callummcdougall/ARENA_2.0/tree/main) about RL to get myself familiar with different components in RL and then spending the rest 25 hours doing the DLC exercise. 

To implement and debug the RL algorithm, I referred to posts [Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html) and [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/).

I used Colab Pro+ environment to enable background running and more compute. The experimentation is done using 1 V100 GPU.

I have generated the  Result [Report](https://wandb.ai/vincentwang25/PPOProcgen/reports/PPO-Implementation-in-Procgen-Env--Vmlldzo0OTQ3NzE5?accessToken=s9w0lpjb2fjv77ouf1c7nrb2s0zcviymc0mmw8pksr34mnsiblw5x7t7izv5gbhs) in Weights & Bias. It shows

1. Increasing amount of episode return
2. reasonable amount of ratios clipped by PPO.
3. Small and fairly stable approximate KL.
4. Policy entropy (relative entropy) falls gradually
5. Value residual Variance (1 - value explained variance) tend to something positive.
6. Mean and standard deviation for advantage normalization are fairly stable and mean is pretty close to zero.

{% include figure.html path="https://i.ibb.co/9rkgjbc/2023-07-23-09-12.png" class="img-fluid rounded z-depth-1" zoomable=true%}

{% include figure.html path="https://i.ibb.co/nQzcTBs/2023-07-23-09-13.png" class="img-fluid rounded z-depth-1" zoomable=true%}



Another closer look at the episode return. It shows that using IMPALA model performs better than traditional CNN, especially at `bigfish` environment.

{% include figure.html path="https://i.ibb.co/2g1Pjfd/2023-07-23-09-16.png" class="img-fluid rounded z-depth-1" zoomable=true%}

Other than these results, I found implementing the PPO algorithm under customized easy probing environments quite helpful. It is also helpful to achieve decent performance under CartPole environment and Atari environments before moving into harder Procgen Environments. 

Extensively track the metrics can also be helpful to debug where things went wrong or well, though relying on some of them solely might be inadequate. I am confused by [residual variance](https://andyljones.com/posts/rl-debugging.html#:~:text=handling%20invalid%20actions.-,Residual%20variance,-The%20variance%20of) oscillation inside CartPole environment before, i.e. the residual variance doesn't go smoothly towards a positive value, but [oscillate wildly](https://wandb.ai//vincentwang25/PPOCart/reports/PPO-CartPPO-CartPole-Wrong-Value-Residual-Variance--Vmlldzo0OTE1NTcw?accessToken=jb0t273joya2ec1a4xaiefydzhb5qd0h1wekl40yo55cr9r6mcz6t25ibj0otim4). It turns out that this can be due to the inherent simplicity of the CartPole environment: it doesn't need much value estimation, but more relying on planning. This cause the value estimation to be unstable.

Overall I found this exercise quite helpful for me to understand the PPO algorithm and generally RL algorithm structure.