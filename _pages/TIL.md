---
layout: page
permalink: /TIL/
title: TIL
description: Today I learnt.
nav: true
nav_order: 5
---
# 2023-08-12 - 2023-08-18
1. Read and submitted my first result to [trojan detection challenge](https://trojandetection.ai/start) , [How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/), 
2. Wrote a blog post about [Red Teaming Language Models with Language Models](https://ziyuewang25.github.io/blog/2023/DLC-T9-AdversarialTraining/)
3. Did my first introductory to EA course.
4. Read many paper/blogs/code about Adversarial Training & Safety Evaluation for [Eval Hackathon](https://alignmentjam.com/jam/evals) , I am currently working on this.

# 2023-08-01 - 2023-08-11
1. Finished DLC Alignment topic exericise, it takes me around 30 hours. The code is here: [Finetune LLM with RLHF to generate positive tone message from Shakespeare Corpus](https://github.com/ZiyueWang25/RLHF-Shakespeare)
2. Finished [Week 3 Goal Misgeneralization](https://course.aisafetyfundamentals.com/alignment?week=3) and [Week 4 Task Decomposition for scalable oversight](https://course.aisafetyfundamentals.com/home/alignment?week=4) reading
3. Wrote blog posts [Find the induction heads in GPT-2](https://ziyuewang25.github.io/blog/2023/DLC-T1-MI/) and [ARENA learning experience](https://ziyuewang25.github.io/blog/2023/ARENA/)

# 2023-07-24 - 2023-07-31
1. In the progress of doing Alignment exercise in [DLC](https://github.com/jacobhilton/deep_learning_curriculum/tree/master)
2. Finished [Week 2 Reward Misspecification](https://course.aisafetyfundamentals.com/alignment?week=2) reading
3. Read [the effectiveness mindset](https://forum.effectivealtruism.org/s/B79ro5zkhndbBKRRX) chapter for the [introductory to EA](https://www.effectivealtruism.org/virtual-programs/introductory-program) course.
4. travel week (Beijing, Shanghai, Hangzhou). Met many friends and had great fun!

# 2023-07-16 - 2023-07-23
1. Around 15 hours to finish [Topic 2 Scaling Laws](https://colab.research.google.com/drive/1xTpfj6xADQYdUudnZE9AWMUzyr8DBoU6?usp=sharing) in [DLC](https://github.com/jacobhilton/deep_learning_curriculum/tree/master).  I wrote a post about it [here](https://ziyuewang25.github.io/blog/2023/DLC-T2-Scaling-Laws/)
2. Around 7 hours to finish [week2_deep_Q_Learning](https://arena-ch2-rl.streamlit.app/).
3. Around 4 hours to finish [week2_PPO](https://colab.research.google.com/drive/1UlhPmIfhQLo_10r5OkDwLxWF--A2iCKc?usp=sharing). The Atari Breakout game result is shown [here](https://wandb.ai//vincentwang25/PPOAtari/reports/videos-23-07-18-13-10-22---Vmlldzo0OTA1MjM0).
4. Around 25 hours to finish Topic 6 Reinforcement Learning in DLC. The solution is written in colab [here](https://colab.research.google.com/drive/1n8EhT0RHxdS1MIgiPQkvjDX7sD7Mpxoy?usp=sharing). The post is written [here](https://ziyuewang25.github.io/blog/2023/DLC-T6-RL/)

# 2023-07-01 - 2023-07-15
Flight back to China. Time with family and friends.

# 2023-06-25 - 2023-06-30
1. Around 2 hours to finish exercise [week0_d4_optimization](https://colab.research.google.com/drive/1HjzmCYqBVz_Q0XVj0mmOZGV26tW1WvHi?usp=sharing). Learnt about details of various optimizer, `Weight & Bias` related usage.
2. Around 2 hours to finish exercise [week0_d5_backprop](https://colab.research.google.com/drive/1WjtXIlpr3iC5fPGC4_hGSm6nYMdIptNc?usp=sharing). Learnt about details of back propagation & Autograd. Skipped part 3 & 4 & 5.
3. Around 3 hours to finish exercise [week1_d1_transformer](https://colab.research.google.com/drive/1Ig779Od-OoO8lHolRqWQTaAvhK98EdJT?usp=sharing). Learnt about details of transformer, sampling, training and inference sampling. Skipped part of sampling.
4. Around 5 hours to finish exercise [week1_d2_mechanistic_interpretability](https://colab.research.google.com/drive/1NfLlt3McxOK9eY4xT_S6Q0ZFaoXtrW2B?usp=sharing). Learnt about induction circuits, transformerLens, induction heads, hooks, reverse-engineering induction circuits. These material opened a new view for me about how to understand LLM. I find some part hard to understand though and skipped some of the exercise as I don't want to spend too much time on this topic for now.
5. Around 3 hours to finish [Topic 8 Interpretability](https://colab.research.google.com/drive/15CSZ09T0LQ4_BAM7_NcGDy5sTVneFJQw?usp=sharing) in [DLC](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/8-Interpretability.md). It didn't take me too long because ARENA courses covered most of it. 
6. Around 2.5 hours to finish [week2_d1_intro_to_RL](https://arena-ch2-rl.streamlit.app/), it works like a memory refresher about some RL concepts. It is nice to check the detail of some RL environments.

# 2023-06-20 - 2023-06-24
1. Around 2.3 hours to finish exercise [week0_d2_cnn](https://colab.research.google.com/drive/1tmwlA1YQIrgXblzo_9q2mNvEOuAqsV3N?usp=sharing). Learnt about `torch.as_strided` related stuff.
2. Around 2.4 hours to finish exercise [week0_d3_resnet](https://colab.research.google.com/drive/1gnUiIAzIvjYvaUXdZP7J8e_n1qeLhHFj?usp=sharing). Learnt a more detailed view about resnet, pytorch_lighting, batchnorm
3. induction head [understanding](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)
4. [Eliciting latent knowledge](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.kkaua0hwmp1d)

# 2023-06-14 - 2023-06-19
1. Followed [Deep Learning Curriculum](https://github.com/jacobhilton/deep_learning_curriculum) and finished [Topic 1 Transformer](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/1-Transformers.md), it takes me about 20 hours but I learnt a lot throughout the process. I wrote a post about it [here](https://ziyuewang25.github.io/blog/2023/DLC-T1-Transformer/)
2. Read Mechanistic Interpretability related materials: [Concrete Steps to Get Started in Transformer Mechanistic Interpretability](https://www.neelnanda.io/mechanistic-interpretability/getting-started) and Google Internal documents about Introduction to Alignment.

# 2023-06-08 - 2023-06-13
1. DDPM and DDIM diffusion Pytorch version replication: [DiffusionModel-DeepLearningAI-Pytorch.ipynb](https://colab.research.google.com/drive/1LcxLcVfxeFQWlEEWVTBx_LfNMaY3CoNg?usp=sharing) . It takes me a while to collect the data to mimic the short [course](https://www.deeplearning.ai/short-courses/how-diffusion-models-work/) on DeepLearning.AI and in the end I decide to use MNIST data with customized label to replicate the results.  The script works well and I am glad to see the model can generate digits according to the given conditions.
2. Kaggle LLM Nerd-Off. Checked out the existing model performance and competition data details. Since the model size constraint is 1B, it seems to me that specialized model to deal with different kind of input is the way to go. Also, parameter-efficient fine-tuning methods are definitely important.
3. Some more readings about AI Safety in [week 1](https://aisafetyfundamentals.com/ai-alignment-curriculum). It is fun to see the [Specification Gaming](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity) issue.
4. [Famine, Affluence, and Morality](https://utilitarianism.net/peter-singer-famine-affluence-and-morality/): it motivates me to donate 10% of my income to do more goods effectively.
5. Consultation with 80k hours works well. I am glad I did it :)

# 2023-06-07
1. Reviewed previous experience about [DeepLOB](https://arxiv.org/abs/1808.03668) and [Kaggle Gravitation Wave Detection Competition](https://github.com/ZiyueWang25/Kaggle_G2Net). Feels like it has been quite a while. I miss the feeling of doing DL models.
2. Reviewed [How Diffusion Models Work](https://www.deeplearning.ai/short-courses/how-diffusion-models-work/) short course again on DeepLearning.AI. I will work on replicating it in Pytorch/JAX. It looks fun!

# 2023-06-01 - 2023-06-06
1. AI Safety related topics:
	1. [Alignment Course - AI Safety Fundamentals](https://aisafetyfundamentals.com/ai-alignment-curriculum): week 0 + week1
	2. [AGI safety from first principles](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view)
	3. Various AI Safety team from different companies: Google DeepMind, OpenAI, Anthropic, etc.
	4. [Preventing an AI-related catastrophe](https://80000hours.org/problem-profiles/artificial-intelligence/?source=email&uni_id=867&utm_source=80%2C000+Hours+mailing+list&utm_campaign=332544752a-EMAIL_CAMPAIGN_2023_06_01_11_29&utm_medium=email&utm_term=0_43bc1ae55c-71ae16e3db-%5BLIST_EMAIL_ID%5D)
	5. [Four Background Claims](https://intelligence.org/2015/07/24/four-background-claims/)
	6. [Visualizing the deep learning revolution](https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5)
2. Google internal Kaggle competition: LLM Nerd-off. First submission :)
3. Deep Learning AI 4 Short [courses](https://www.deeplearning.ai/short-courses/)
4. 1 on 1 consultation [chat](https://80000hours.org/speak-with-us/?int_campaign=homepage__get-1-1-advice) with 80k hours.
5. Finish reading [Doing Good Better: How Effective Altruism Can Help You Make a Difference](https://www.goodreads.com/book/show/23398748-doing-good-better)
6. Reading [The alignment problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem)

# 2023-05-25
1. [podcast: PhD or programming?Fast paths into aligning AI as a machine learning engineer, according to ML engineers Catherine Olsson & Daniel Ziegler](https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/): dive in, write detail plan and ask mentor to guide
2. [Dive In](https://mindingourway.com/dive-in-2/):
	1. [conviction without self-deception](https://mindingourway.com/conviction-without-s/): stop conflating feelings with beliefs.
	2. [deliberate once](https://mindingourway.com/deliberate-once/): about making big decision, deliberate once and then don't deliberate again until new information comes in that would have changed the result of the deliberation.
		1. commitment-aversion thoughts
		2. abort notice
		3. confusion pings
	3. realize that you find good things to do by getting your hands dirty

# 2023-05-20 - 2023-05-23
1. Virtual Try-on models:
	1. [TryOnDiffusion: A Tale of Two UNets](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf): parallel-UNet, cross-attention, cascade diffusion, FiLM, Efficient UNet. Amazing results!
	2. [High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions](https://arxiv.org/abs/2206.14180): classic Virtual Try On (VITON) model by warping the cloth first and then blend with the target person.
	3. [TryOnGAN](https://tryongan.github.io/tryongan/)
	4. [Single Stage Virtual Try-on via Deformable Attention Flows](https://arxiv.org/abs/2207.09161)
2. [FiLM](https://distill.pub/2018/feature-wise-transformations/): a way to combine concatenation & multiplication feature transformation.
3. Diffusion Model Related read:
	1. [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) by Lilian Weng
	2. [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) by huggingface
	3. [Diffusion Models: A Comprehensive Survey of Methods and Applications](https://arxiv.org/abs/2209.00796)
	4. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239): classic DDPM algorithm
	5. [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672): learning variance of the reverse diffusion process.
	6. [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502): DDIM , a more efficient iterative process for the reverse diffusion part.
	7. [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233): classifier guidance
	8. [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2104.07636): classifier-free guidance
	9. [High-Resolution Image Synthesis with Latent Diffusion Models](https://ieeexplore.ieee.org/document/9878449): cross attention.
	10. [Image Super-Resolution via Iterative Refinement](https://arxiv.org/abs/2104.07636)
	11. [Cascaded Diffusion Models for High Fidelity Image Generation](https://arxiv.org/abs/2106.15282)
	12. [Palette: Image-to-Image Diffusion Models](https://arxiv.org/abs/2111.05826): image to image diffusion model
	13. [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487): text-to-image model by leveraging LLM.
4. Human pose and segmentation:
	1. [Towards Accurate Multi-person Pose Estimation in the Wild](https://arxiv.org/abs/1701.01779)
	2. [Graphonomy: Universal Human Parsing via Graph Transfer Learning](https://arxiv.org/abs/1904.04536)
5. [Parti](https://sites.research.google/parti/): Pathways Autoregressive Text-to-Image model 


# 2023-05-17 - 2023-05-19
1. Replicating [Attention is all you need](https://arxiv.org/abs/1706.03762) paper in JAX/Flax/Optax.

# 2023-05-16
1. [How to pursue a career in technical AI alignment](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment): this is amazing post about ML/DL/AI alignment.

# 2023-05-15
1. [Leverage Dopamine to Overcome Procrastination & Optimize Effort](https://hubermanlab.com/leverage-dopamine-to-overcome-procrastination-and-optimize-effort/): this podcast episode is interesting and informative. The biggest takeaway for me is to understand how our motivation level changes according to dopamine and what the dopamine circuit is like. It is also surprising to learn that we can leverage something that sucks to overcome procrastination. A good summary can be found [here](https://podcastnotes.org/huberman-lab/leverage-dopamine-to-overcome-procrastination-optimize-effort-huberman-lab/)
2. Instead of reading paper, I should spend more time getting my hands dirty. It is harder and has more direct impact on my career.

# 2023-05-14
1. [NeRF - Representing Scenes as Neural Radiance Fields for View Synthesis](https://www.matthewtancik.com/nerf): First time tackling 3D image synthesis problem. Followed [tiny_nerf.ipynb](https://github.com/bmild/nerf/blob/master/tiny_nerf.ipynb) notebook and added hierarchical sampling on it. [tiny_nerf_with_hierarchical_sampling.ipynb](https://colab.research.google.com/drive/1U18TrmcnL1TpZj1R-qv43hsxfu_DizOc?usp=sharing)
2. [NeRF in the Wild](https://nerf-w.github.io/): add a series of extensions to NeRF to address dynamic subjects captured under uncontrolled setting.

# 2023-05-13
1. [Project Starline](https://blog.google/technology/research/project-starline/): 3D display to connect people. [video](https://www.youtube.com/watch?v=kDgToq5aXh0&ab_channel=GoogleAR%26VR)
2. [Novel View Synthesis with Diffusion Models](https://3d-diffusion.github.io/): 3D generation from a few image pairs.
3. [NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images](https://bmild.github.io/rawnerf/): 
4. [Time-Travel Rephotography](https://time-travel-rephotography.github.io/): use styleGAM2 framework to project old photo into modern high-resolution photos
5. [HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields](https://hypernerf.github.io/)

# 2023-05-12
1. [Whisper](https://openai.com/research/whisper): English speech recognition
2. [Zero Redundancy Optimizer](https://www.deepspeed.ai/tutorials/zero/) :partitioning various model training states across the available devices
3. [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://bigscience.huggingface.co/blog/bloom)
4. I got a personal poem written by Melanie Reed when we walk around Green Lake. It is beautiful and touching.

# 2023-05-11
1. [80,000 hours](https://80000hours.org/): 80,000 hours in your career. The [key ideas](https://80000hours.org/key-ideas/) page is amazing.
2. [PaLM2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/)
3. [UL2](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html): A way to include encoder like and decoder like training objective together.
4. Some exploration about green card application @ Australia, Germany, Netherlands and Sweden. Be mentally prepared for the Chinese discrimination in the USA.

# 2023-05-10 
1. [Why did all of the public reproduction of GPT-3 fail? In which tasks should we use GPT-3.5/ChatGPT?](https://jingfengyang.github.io/gpt): A practical point of view to compare different LLM reproduction and application issue.
2. [ImageBind](https://imagebind.metademolab.com/): wow, the idea is pretty straightforward: learning the joint embedding from multiple modalities. Using image as the central embedding. Interesting!

# 2023-05-09
1. [Harnessing the power of LLMs in Practice](https://github.com/Mooler0410/LLMsPracticalGuide): the chart inside is quite informative and the future trend is also thoughtful: evaluation on real-world datasets, alignment, safety.
2. To do great things, focus on small stuff. 
3. To make a fortune, help 1-billion people or make a grand breakthrough.

# 2023-05-08
1. [Now Page](https://nownownow.com/): A simple but quite informative and fun page created by Derek Sivers. 
2. [DataComp](https://www.datacomp.ai/): A framework about improving dataset by fixing model.
   1. [Detoxify](https://github.com/unitaryai/detoxify): model to remove unsafe content.
   2. Deduplication using constrastive learning. [link](https://arxiv.org/pdf/2112.04323.pdf)
   3. Face detection and blurring for privacy. [detection model](https://arxiv.org/abs/2105.04714). [blurring effect](https://arxiv.org/abs/2103.06191)
3. We definitely need to advocate for ourselves.
4. Cleaning up the system can be time-consuming and a headache.

